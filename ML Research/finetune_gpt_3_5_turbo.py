# -*- coding: utf-8 -*-
"""Finetune GPT 3.5 Turbo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pWDeJr2r149vpoL7cVEXWqi4LaB1WHxF
"""

#From the freely available GPT models for finetuning, the GPT 3.5 turbo is the best narrowed down LLM for out use case.
import os
import openai
openai.api_key = os.getenv("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")
file = openai.File.create(
    file=open("scienceqa_train.jsonl", "rb"),
    purpose='fine-tune',
)

!pip install openai

def format_chat(row):
    return json.dumps(
        {"messages": [
            {"role": "user", "content": row["prompt"]},
            {"role": "assistant", "content": str(row["response"])},
         ]}
    )
def convert_dataset(df, file_name):
    df["conversation"] = df.apply(format_chat, axis=1),
    with open(file_name, 'w') as jsonl_file:
        for example in df["conversation"]:
            jsonl_file.write(example + '\n')

#Converting the pd dataframe to JSONL compatible object.
import json
import pandas as pd

DEFAULT_SYSTEM_PROMPT = 'You are a financial banking assistant for customers. You should help the user to answer his question.'

def create_dataset(question, answer):
    return {
        "messages": [
            {"role": "system", "content": DEFAULT_SYSTEM_PROMPT},
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer},
        ]
    }

if __name__ == "__main__":
    df = pd.read_csv("/content/data_val.csv", encoding='cp1252')
    with open("train_val.jsonl", "w") as f:
        for _, row in df.iterrows():
            example_str = json.dumps(create_dataset(row["Question"], row["Answer"]))
            f.write(example_str + "\n")

import os
import openai

os.environ["OPENAI_API_KEY"] = "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"
openai.api_key = os.environ["OPENAI_API_KEY"]

res = openai.File.create(
    file=open("/content/train.jsonl", "r"),
    purpose='fine-tune'
)
res

res = openai.File.create(
    file=open("/content/train_val.jsonl", "r"),
    purpose='fine-tune'
)
res

openai.File.retrieve("file-ijMm7Ir46yUdmKyQyRdcRQ3V")

train_data = "file-ijMm7Ir46yUdmKyQyRdcRQ3V"
val_data = "file-fZd4ZrX1wO9CxI4a4zmjRfsL"
model = openai.FineTuningJob.create(
    model = "gpt-3.5-turbo",
    training_file = train_data,
    validation_file = val_data,
    suffix = "scienceqa"
)

!pip install langchain

from llama_index.finetuning import OpenAIFinetuneEngine

finetune_engine = OpenAIFinetuneEngine(
    "gpt-3.5-turbo",
    "finetuning_events.jsonl",
    # start_job_id="<start-job-id>"  # if you have an existing job, can specify id here
)

#Integrating the finetuned_engine with our educator.. Part of future expansion of the project, keeping in mind the scalability requirements.